---
layout: default
---
[Home](./README.md).
[Internet](./internet.md).
[Cloud Computing](./cloud_computing.md).
[Serverless](./serverless.md).
[AWS](./aws.md).
[Spark Overview](./spark_overview.md).
[Spark DataFrame & SQL API](./sparkAPI.md).
[Spark Internals](./spark_internals.md).
[Advanced Optimizations in Spark](spark_optimizations.md).
[Help/Resources](./resources.md).

### Serverless Solutions in the Cloud

### Introduction
Data on the web is growing exponentially. All of us today use Google as our first source of knowledge – be it about finding reviews about a place to understanding a new term. All the data that you need is already available on the internet – the only thing limiting you from using it is the ability to access it. However, most of the data available over the web is not readily available. It is present in an unstructured format (HTML format) and is not downloadable. Therefore, it requires knowledge & expertise to use this data to eventually build a useful model.

#### Web Scraping
Web scraping is a technique for converting the data present in unstructured format (HTML tags) over the web to the structured format which can easily be accessed and used.

#### How to Scrape?
**There are several ways of scraping data from the web. Some of the popular ways are:**

- Human Copy-Paste: This is a slow and efficient way of scraping data from the web. This involves humans themselves analyzing and copying the data to local storage.

- Text pattern matching: Another simple yet powerful approach to extract information from the web is by using regular expression matching facilities of programming languages.

- API Interface: Many websites like Facebook, Twitter, LinkedIn, etc. provides public and/ or private APIs which can be called using the standard code for retrieving the data in the prescribed format.

- DOM Parsing: By using web browsers, programs can retrieve the dynamic content generated by client-side scripts. It is also possible to parse web pages into a DOM tree, based on which programs can retrieve parts of these pages. *We are going to use this.*

#### [Web Scraping Demo](https://github.com/tidyverse/rvest/tree/master/demo)
```r

library(rvest)
the_shawshank_redemption <- read_html("https://www.imdb.com/title/tt0111161/")

cast <- the_shawshank_redemption %>%
  html_nodes("#titleCast .primary_photo img") %>%
  html_attr("alt")
cast

```
**Output:** 
 [1] "Tim Robbins" "Morgan Freeman" "Bob Gunton" "William Sadler" "Clancy Brown"  "Gil Bellows"  "Mark Rolston" [8] "James Whitmore" "Jeffrey DeMunn" "Larry Brandenburg" "Neil Giuntoli" "Brian Libby"  "David Proval" "Joseph Ragno"  [15] "Jude Ciccolella"   

![scrapedemo1](Images/Serverless/cast.png)

### rvest
- rvest is package that makes it easy to scrape data from html web pages, inspired by libraries like beautifulsoup (bs4). You can express complex operations as elegant pipelines composed of simple, easily understood pieces. rvest is built upon the xml2 package and also accept config from the httr package. For the most part, we only need rvest. However, we need httr if we want to add extra configurations.

**Make sure you have this package installed**
```r
install.packages("rvest")
```

```r
install.packages("httr")
```

- We will be using an open source software named Selector Gadget which will be more than sufficient for anyone in order to perform Web scraping. You can access and **download the Selector Gadget extension** [here](https://selectorgadget.com/). Make sure that you have this extension installed by following the instructions from the website. 

>Using this you can select the parts of any website and get the relevant tags to get access to that part by simply clicking on that part of the website. Note that, this is a way around to actually learning HTML & CSS and doing it manually. But to master the art of Web scraping, I’ll highly recommend you to learn HTML & CSS in order to better understand and appreciate what’s happening under the hood.


## Web Scraping Example
```r

#Loading the rvest package
library(rvest)

#Specifying the url for desired website to be scraped
url <- 'https://www.imdb.com/search/title/?count=100&release_date=1994,1994&title_type=feature'

#Reading the HTML code from the website
webpage <- read_html(url)

```

**Now let's scrape IMDB:**
- *Rank:* The rank of the film from 1 to 100 on the list of 100 most popular feature films released in 1994
- *Title:* The title of the feature film.
- *Description:* The description of the feature film.
- *Runtime:* The duration of the feature film.
- *Genre:* The genre of the feature film,
- *Rating:* The IMDb rating of the feature film.
- *Director:* The main director of the feature film. Note, in case of multiple directors, I’ll take only the first.
- *Actor:* The main actor in the feature film. Note, in case of multiple actors, I’ll take only the first.

#### STEP 0: Check Robots.txt

https://www.imdb.com/robots.txt

Good bots comply to the rules set by websites in their robots.txt file and follow best practices while crawling and scraping. 

#### STEP 1: Scraping the Rank field
For that, we’ll use the selector Google Chrome extension gadget that you've downloaded already to get the specific CSS selectors that encloses the rankings. You can click on the extension in your browser and select the rankings field with the cursor. 

Make sure that all the rankings are selected. You can select some more ranking sections in case you are not able to get all of them and you can also de-select them by clicking on the selected section to make sure that you only have those sections highlighted that you want to scrape for that go.

![cssselector](Images/Serverless/selector1.png)

Once you are sure that you have made the right selections, you need to copy the corresponding CSS selector.
```r
#Using CSS selectors to scrape the rankings section
rank_data_html <- html_nodes(webpage,'.text-primary')

#Converting the ranking data to text
rank_data <- html_text(rank_data_html)

#Data-Preprocessing: Converting rankings to numerical
rank_data<-as.numeric(rank_data)

#Let's have a look at the rankings
head(rank_data)

```
*Output: 1, 2, 3, 4, 5, 6*

#### STEP 2: Scraping the title field. 
Again, I have the corresponding CSS selector for the titles – .lister-item-header a. I will use this selector to scrape all the titles using the following code.
```r
#Using CSS selectors to scrape the title section
title_data_html <- html_nodes(webpage,'.lister-item-header a')

#Converting the title data to text
title_data <- html_text(title_data_html)

#Let's have a look at the title
head(title_data)

```
*Output: "The Shawshank Redemption" "Pulp Fiction" "The Lion King" "Forrest Gump" " Léon: The Professional" " Interview with the Vampire: The Vampire Chronicles"*

![selectorcss2](Images/Serverless/selector2.png)

#### STEP 2: Scraping the title description. 
```r
#Using CSS selectors to scrape the description section
description_data_html <- html_nodes(webpage,'.ratings-bar+ .text-muted')

#Converting the description data to text
description_data <- html_text(description_data_html)

#Data-Preprocessing: removing '\n'
description_data<-gsub("\n","",description_data)

#Let's have a look at the description data
head(description_data)
```
*Output: [1] "    Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency."                                                                                          
[2] "    The lives of two mob hitmen, a boxer, a gangster and his wife, and a pair of diner bandits intertwine in four tales of violence and redemption."                                                                 
[3] "    A Lion cub crown prince is tricked by a treacherous uncle into thinking he caused his father's death and flees into exile in despair, only to learn in adulthood his identity and his responsibilities."         
[4] "    The presidencies of Kennedy and Johnson, the events of Vietnam, Watergate, and other history unfold through the perspective of an Alabama man with an IQ of 75."                                                 
[5] "    Mathilda, a 12-year-old girl, is reluctantly taken in by Léon, a professional assassin, after her family is murdered. An unusual relationship forms as she becomes his protégée and learns the assassin's trade."
[6] "    A vampire tells his epic life story: love, betrayal, loneliness, and hunger."*

![cssselector3](Images/Serverless/selector3.png)

#### STEP 3: Scraping the runtime field. 
```r
#Using CSS selectors to scrape the Movie runtime section
runtime_data_html <- html_nodes(webpage,'.text-muted .runtime')

#Converting the runtime data to text
runtime_data <- html_text(runtime_data_html)

#Data-Preprocessing: removing mins and converting it to numerical

runtime_data<-gsub(" min","",runtime_data)
runtime_data<-as.numeric(runtime_data)

#Let's have a look at the runtime
head(runtime_data)

```
*Output: 142 154  88 142 110 123*

#### STEP 4: Scraping the genre field. 
```r
#Using CSS selectors to scrape the Movie genre section
genre_data_html <- html_nodes(webpage,'.genre')

#Converting the genre data to text
genre_data <- html_text(genre_data_html)

#Data-Preprocessing: removing \n
genre_data<-gsub("\n","",genre_data)

#Data-Preprocessing: removing excess spaces
genre_data<-gsub(" ","",genre_data)

#taking only the first genre of each movie
genre_data<-gsub(",.*","",genre_data)

#Convering each genre from text to factor
genre_data<-as.factor(genre_data)

#Let's have another look at the genre data
head(genre_data)

```
*Output:  Drama     Crime     Animation Drama     Action    Drama    
Levels: Action Adventure Animation Biography Comedy Crime Drama Family Fantasy Mystery*

#### STEP 5: Scraping the rating field. 
```r
#Using CSS selectors to scrape the IMDB rating section
rating_data_html <- html_nodes(webpage,'.ratings-imdb-rating strong')

#Converting the ratings data to text
rating_data <- html_text(rating_data_html)

#Data-Preprocessing: converting ratings to numerical
rating_data<-as.numeric(rating_data)

#Let's have another look at the ratings data
head(rating_data)

```
*Output: 9.3 8.9 8.5 8.8 8.5 7.6*

#### STEP 6: Scraping the Director field. 
```r
#Using CSS selectors to scrape the directors section
directors_data_html <- html_nodes(webpage,'.text-muted+ p a:nth-child(1)')

#Converting the directors data to text
directors_data <- html_text(directors_data_html)

#Data-Preprocessing: converting directors data into factors
directors_data<-as.factor(directors_data)

#Let's have a look at the directors data
head(directors_data)
```
*Output: Frank Darabont    Quentin Tarantino Roger Allers      Robert Zemeckis   Luc Besson        Neil Jordan*      
*99 Levels: Alex Proyas Andrew Bergman Andrew Fleming Andrew Morahan Atom Egoyan Barry Levinson Béla Tarr Ben Stiller Bernard Rose ... William Dear*

#### STEP 2: Scraping the Actor field. 
```r
#Using CSS selectors to scrape the actors section
actors_data_html <- html_nodes(webpage,'.lister-item-content .ghost+ a')

#Converting the gross actors data to text
actors_data <- html_text(actors_data_html)

#Data-Preprocessing: converting actors data into factors
actors_data<-as.factor(actors_data)

#Let's have a look at the actors data
head(actors_data)
```
*Tim Robbins       John Travolta     Matthew Broderick Tom Hanks         Jean Reno         Brad Pitt        
89 Levels: Alec Baldwin Alex Hyde-White Arnold Schwarzenegger Brad Pitt Brandon Lee Brendan Fraser Brian Bonsall ... Zbigniew Zamachowski*

#### STEP 9: Creating a DataFrame 
Now we have successfully scraped all the features for the 100 most popular feature films released in 1994. Let’s combine them to create a dataframe and inspect its structure.
```r
#Combining all the lists to form a data frame
movies_df<-data.frame(Rank = rank_data, Title = title_data,
                      Description = description_data, Runtime = runtime_data,
                      Genre = genre_data, Rating = rating_data,
                      Director = directors_data, 
                      Actor = actors_data)

#Structure of the data frame
view(movies_df)
```
![dataframe](Images/Serverless/dfmovies.png)


#### STEP 10: Simple plot
```r
library('ggplot2')

qplot(data = movies_df, Genre, fill = Genre)
```

```r
ggplot(movies_df,aes(x=Runtime,y=Rank))+
geom_point(aes(size=Rating,col=Genre))
```
![dataframe](Images/Serverless/gplot1.png)



## More on rvest
####  <a name="rvest2">Making Simple Requests</a>

rvest provides two ways of making request: `read_html()` and `html_session()`  
`read_html()` can parse a HTML file or an url into xml document. `html_session()` is built on `GET()` from httr package and can accept configurations any additonal httr config.  

Reading a URL:

```R
# making GET request and parse website into XML document
pagesource <- html_read("http://example.com/page")

# Using html_session which creates a session and accept httr methods
my_session <- html_session("http://example.com/page")
#html_session is built upon httr, you can also get response with a session
response <- my_session$response
```

Alternatively, GET and POST method are available in the httr package.

```R
library(httr)
response <- GET("http://example.com/page")
#or
response <- POST("http://example.com/page",
    body = list(a=1,b=2))
```

## <a name="rvest3">Inspecting Response</a>

Check status code:

```R
status_code(my_session)
status_code(response)
```

Get response and content:

```R
#response
response <- my_session$response
#retrieve content as raw
content_raw <- content(my_session$response,as = "raw")
#retrieve content as text
content_text <- content(my_session$response,as = "text")
#retrieve content as parsed(parsed automatically)
content_parsed <- content(my_session$response,as = "parsed")
```

Content may be parsed incorrectly sometimes. For those situations, you can parse the content to text or raw and use other libraries or functions to parse it correctly.

Search for specific string:

```R
library(stringr)
#regular expression can also be used here
if(str_detect(content_text,"blocked")){
    print("blocked from website")
    }
```

check content type:

```R
response$headers$`content-type`
```

check html structure:

```R
my_structure <- html_structure(content_parsed)
```

### AWS Comprehend & Polly & Recognizer (The Shawshank Redemption - Demo)


* * *
* * *

#### Sourcers/Credits:
To be completed
